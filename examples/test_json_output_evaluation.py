"""
Test Suite: √âvaluation de la capacit√© du mod√®le √† g√©rer les sorties JSON
(Structure et s√©mantique)

Cette suite de tests √©value progressivement la capacit√© du mod√®le √†:
- G√©n√©rer des structures JSON valides
- Respecter les sch√©mas Pydantic
- Extraire et structurer des donn√©es financi√®res complexes
- G√©rer des cas de plus en plus difficiles (test 1 = facile, test 10 = tr√®s difficile)

Un juge automatique √©value chaque r√©ponse et fournit un score consolid√©.
"""

import asyncio
import json
import traceback
from typing import Any, Dict, List, Optional, get_args, get_origin
from pydantic import BaseModel, Field, ValidationError
from pydantic_ai import Agent, ModelSettings
from pydantic_ai.exceptions import ToolRetryError

from app.models import finance_model


# ============================================================================
# MOD√àLES DE DONN√âES POUR LES TESTS (progression de complexit√©)
# ============================================================================

# Test 1: Simple - Position unique
class SimplePosition(BaseModel):
    """Position boursi√®re simple."""
    symbole: str
    quantite: int = Field(ge=0)
    prix: float = Field(ge=0)


# Test 2: Liste simple
class SimplePortfolio(BaseModel):
    """Portfolio avec liste simple."""
    positions: list[SimplePosition]
    valeur_totale: float = Field(ge=0)


# Test 3: Nested - Position avec d√©tails
class PositionDetaillee(BaseModel):
    """Position avec informations d√©taill√©es."""
    symbole: str
    quantite: int = Field(ge=0)
    prix_achat: float = Field(ge=0)
    date_achat: str
    secteur: str
    pays: str


class PortfolioDetaille(BaseModel):
    """Portfolio avec positions d√©taill√©es."""
    positions: list[PositionDetaillee]
    valeur_totale: float = Field(ge=0)
    date_evaluation: str


# Test 4: Nested avec arrays - Performance
class PerformanceMensuelle(BaseModel):
    """Performance mensuelle."""
    mois: str
    rendement: float
    volatilite: float = Field(ge=0)


class PositionAvecPerformance(BaseModel):
    """Position avec historique de performance."""
    symbole: str
    quantite: int = Field(ge=0)
    prix_achat: float = Field(ge=0)
    performances: list[PerformanceMensuelle]


# Test 5: Multiple related objects
class Transaction(BaseModel):
    """Transaction financi√®re."""
    type: str  # "achat" ou "vente"
    symbole: str
    quantite: int = Field(ge=0)
    prix: float = Field(ge=0)
    date: str
    frais: float = Field(ge=0, default=0.0)


class HistoriqueTransactions(BaseModel):
    """Historique de transactions."""
    transactions: list[Transaction]
    total_achats: float = Field(ge=0)
    total_ventes: float = Field(ge=0)
    solde_net: float


# Test 6: Deep nesting - Portfolio avec analyse
class AnalyseRisque(BaseModel):
    """Analyse de risque d'une position."""
    volatilite: float = Field(ge=0)
    beta: float
    sharpe_ratio: float
    var_95: float = Field(ge=0)


class PositionAnalysee(BaseModel):
    """Position avec analyse compl√®te."""
    symbole: str
    quantite: int = Field(ge=0)
    prix_achat: float = Field(ge=0)
    prix_actuel: float = Field(ge=0)
    analyse_risque: AnalyseRisque
    performance: dict[str, float]  # {"1m": 0.05, "3m": 0.12, "1y": 0.25}


class PortfolioAnalyse(BaseModel):
    """Portfolio avec analyse compl√®te."""
    positions: list[PositionAnalysee]
    analyse_globale: dict[str, float]
    recommandations: list[str]


# Test 7: Mixed types and optional fields
class MetriqueOptionnelle(BaseModel):
    """M√©trique optionnelle."""
    nom: str
    valeur: float
    unite: Optional[str] = None
    commentaire: Optional[str] = None


class PositionFlexible(BaseModel):
    """Position avec champs optionnels."""
    symbole: str
    quantite: int = Field(ge=0)
    prix_achat: float = Field(ge=0)
    prix_actuel: Optional[float] = None
    metriques: Optional[list[MetriqueOptionnelle]] = None
    tags: Optional[list[str]] = None


# Test 8: Complex financial calculations
class CalculFinancier(BaseModel):
    """R√©sultat de calcul financier."""
    type_calcul: str  # "valeur_future", "versement_mensuel", etc.
    parametres: dict[str, Any]
    resultat: float
    details: dict[str, float]
    validation: bool = Field(description="True si le calcul est coh√©rent")


class AnalyseCalculs(BaseModel):
    """Analyse avec calculs financiers."""
    calculs: list[CalculFinancier]
    conclusion: str
    confiance: float = Field(ge=0.0, le=1.0)


# Test 9: Multi-step extraction with relationships
class RelationPosition(BaseModel):
    """Relation entre positions."""
    position_source: str
    position_cible: str
    type_relation: str  # "correlation", "hedge", "diversification"
    force: float = Field(ge=-1.0, le=1.0)


class PortfolioRelationnel(BaseModel):
    """Portfolio avec relations entre positions."""
    positions: list[PositionDetaillee]
    relations: list[RelationPosition]
    clusters: Optional[dict[str, list[str]]] = None
    strategie: str


# Test 10: Full portfolio analysis (most complex)
class MetriqueAvancee(BaseModel):
    """M√©trique avanc√©e."""
    nom: str
    valeur: float
    historique: list[float]
    tendance: str  # "hausse", "baisse", "stable"
    seuil_alerte: Optional[float] = None


class AnalyseComplete(BaseModel):
    """Analyse compl√®te du portfolio."""
    positions: list[PositionAnalysee]
    metriques_globales: dict[str, MetriqueAvancee]
    analyse_risque: AnalyseRisque
    recommandations: list[dict[str, Any]]
    scenarios: Optional[list[dict[str, Any]]] = None
    date_analyse: str
    version_modele: str = "1.0"


# ============================================================================
# AGENT POUR EXTRACTION
# ============================================================================

extract_agent = Agent(
    finance_model,
    model_settings=ModelSettings(max_output_tokens=3000),
    system_prompt=(
        "Vous √™tes un assistant expert en analyse de donn√©es financi√®res. "
        "Votre r√¥le est d'extraire des informations structur√©es √† partir "
        "de textes non structur√©s concernant des portfolios d'actions fran√ßaises. "
        "Vous devez TOUJOURS r√©pondre avec un JSON valide qui respecte exactement "
        "le sch√©ma demand√©. V√©rifiez que tous les champs requis sont pr√©sents "
        "et que les types de donn√©es sont corrects.\n\n"
        "IMPORTANT: R√©pondez UNIQUEMENT avec du JSON valide. Ne commencez pas par du texte, "
        "ne finissez pas par du texte, et ne mettez pas le JSON dans un bloc de code markdown."
    ),
)


# ============================================================================
# JUGES D'√âVALUATION
# ============================================================================

class JudgeResult:
    """R√©sultat d'√©valuation d'un test."""
    def __init__(self, test_num: int, test_name: str, structure_valid: bool, schema_valid: bool,
                 semantics_valid: bool, completeness_score: float, correctness_score: float,
                 overall_score: float, errors: List[str], warnings: List[str],
                 details: Dict[str, Any]):
        self.test_num = test_num
        self.test_name = test_name
        self.structure_valid = structure_valid
        self.schema_valid = schema_valid
        self.semantics_valid = semantics_valid
        self.completeness_score = completeness_score
        self.correctness_score = correctness_score
        self.overall_score = overall_score
        self.errors = errors
        self.warnings = warnings
        self.details = details


class JSONJudge:
    """Juge pour √©valuer les sorties JSON."""
    
    def __init__(self):
        self.results: list[JudgeResult] = []
    
    def evaluate(
        self,
        test_num: int,
        test_name: str,
        expected_model: type[BaseModel],
        response_text: str,
        expected_fields: Optional[list[str]] = None
    ) -> JudgeResult:
        """√âvalue une r√©ponse JSON."""
        errors = []
        warnings = []
        structure_valid = False
        schema_valid = False
        semantics_valid = False
        completeness_score = 0.0
        correctness_score = 0.0
        details = {}
        
        # 1. V√©rifier que c'est du JSON valide
        try:
            json_data = json.loads(response_text)
            structure_valid = True
            details["json_parsed"] = True
        except json.JSONDecodeError as e:
            errors.append(f"JSON invalide: {str(e)}")
            details["json_parsed"] = False
            details["json_error"] = str(e)
            return JudgeResult(
                test_num=test_num,
                test_name=test_name,
                structure_valid=False,
                schema_valid=False,
                semantics_valid=False,
                completeness_score=0.0,
                correctness_score=0.0,
                overall_score=0.0,
                errors=errors,
                warnings=warnings,
                details=details
            )
        
        # 2. V√©rifier le sch√©ma Pydantic
        try:
            validated_data = expected_model.model_validate(json_data)
            schema_valid = True
            details["schema_validated"] = True
            details["validated_data"] = validated_data.model_dump()
        except ValidationError as e:
            schema_valid = False
            errors.append(f"Sch√©ma invalide: {str(e)}")
            details["schema_validated"] = False
            details["validation_errors"] = [str(err) for err in e.errors()]
        
        # 3. V√©rifier la compl√©tude (champs requis pr√©sents)
        if schema_valid and expected_fields:
            model_fields = set(expected_model.model_fields.keys())
            provided_fields = set(json_data.keys()) if isinstance(json_data, dict) else set()
            missing_fields = set(expected_fields) - provided_fields
            if missing_fields:
                warnings.append(f"Champs manquants: {missing_fields}")
            completeness_score = 1.0 - (len(missing_fields) / len(expected_fields))
        elif schema_valid:
            completeness_score = 1.0
        
        # 4. V√©rifier la s√©mantique (logique m√©tier)
        if schema_valid:
            semantics_valid = self._check_semantics(json_data, expected_model)
            if not semantics_valid:
                errors.append("Erreurs s√©mantiques d√©tect√©es")
            correctness_score = 1.0 if semantics_valid else 0.7
        
        # 5. Calculer le score global
        if not structure_valid:
            overall_score = 0.0
        elif not schema_valid:
            overall_score = 0.3
        elif not semantics_valid:
            overall_score = 0.6
        else:
            overall_score = (completeness_score * 0.3 + correctness_score * 0.7)
        
        return JudgeResult(
            test_num=test_num,
            test_name=test_name,
            structure_valid=structure_valid,
            schema_valid=schema_valid,
            semantics_valid=semantics_valid,
            completeness_score=completeness_score,
            correctness_score=correctness_score,
            overall_score=overall_score,
            errors=errors,
            warnings=warnings,
            details=details
        )
    
    def _check_semantics(self, data: Any, model: type[BaseModel]) -> bool:
        """V√©rifie la s√©mantique des donn√©es en utilisant les m√©tadonn√©es du mod√®le."""
        if isinstance(data, dict):
            model_fields = model.model_fields
            # Obtenir le sch√©ma JSON du mod√®le une seule fois pour toutes les v√©rifications
            try:
                model_schema = model.model_json_schema()
                properties_schema = model_schema.get('properties', {})
            except (AttributeError, TypeError):
                properties_schema = {}
            
            for key, value in data.items():
                # V√©rifier les contraintes du champ si pr√©sent dans le mod√®le
                if key in model_fields:
                    field_info = model_fields[key]
                    
                    # V√©rifier les contraintes num√©riques (ge, gt, le, lt) depuis Field
                    if isinstance(value, (int, float)):
                        # Utiliser le sch√©ma JSON du mod√®le pour obtenir les contraintes du champ
                        field_schema = properties_schema.get(key, {})
                        
                        # Check ge (greater than or equal) / minimum
                        if 'minimum' in field_schema:
                            if value < field_schema['minimum']:
                                return False
                        # Check gt (greater than) / exclusiveMinimum
                        if 'exclusiveMinimum' in field_schema:
                            if value <= field_schema['exclusiveMinimum']:
                                return False
                        # Check le (less than or equal) / maximum
                        if 'maximum' in field_schema:
                            if value > field_schema['maximum']:
                                return False
                        # Check lt (less than) / exclusiveMaximum
                        if 'exclusiveMaximum' in field_schema:
                            if value >= field_schema['exclusiveMaximum']:
                                return False
                    
                    # G√©rer les listes avec validation r√©cursive bas√©e sur le type r√©el
                    if isinstance(value, list):
                        # Extraire le type des √©l√©ments de la liste depuis les annotations du mod√®le
                        field_annotation = model.model_fields[key].annotation
                        origin = get_origin(field_annotation)
                        
                        if origin is list or origin is List:
                            item_type = get_args(field_annotation)[0] if get_args(field_annotation) else None
                            
                            for item in value:
                                # Si l'√©l√©ment est d√©j√† une instance BaseModel, utiliser son type
                                if isinstance(item, BaseModel):
                                    if not self._check_semantics(item.model_dump(), type(item)):
                                        return False
                                # Si l'√©l√©ment est un dict et qu'on a un type BaseModel, l'utiliser
                                elif isinstance(item, dict) and item_type:
                                    try:
                                        if issubclass(item_type, BaseModel):
                                            if not self._check_semantics(item, item_type):
                                                return False
                                        else:
                                            # Type non-BaseModel, validation g√©n√©rique
                                            if not self._check_semantics(item, model):
                                                return False
                                    except TypeError:
                                        # item_type n'est pas une classe, validation g√©n√©rique
                                        if not self._check_semantics(item, model):
                                            return False
                                # Sinon, validation r√©cursive g√©n√©rique
                                elif isinstance(item, (dict, list)):
                                    # Essayer d'utiliser item_type si c'est un BaseModel
                                    nested_model = model
                                    if item_type:
                                        try:
                                            if issubclass(item_type, BaseModel):
                                                nested_model = item_type
                                        except TypeError:
                                            pass
                                    if not self._check_semantics(item, nested_model):
                                        return False
                
                # Pour les champs non d√©finis dans le mod√®le mais pr√©sents dans les donn√©es
                # (par exemple dans des dict g√©n√©riques), validation r√©cursive g√©n√©rique
                elif isinstance(value, (dict, list)):
                    if not self._check_semantics(value, model):
                        return False
        
        # Si les donn√©es sont une instance BaseModel, valider avec son type
        elif isinstance(data, BaseModel):
            return self._check_semantics(data.model_dump(), type(data))
        
        return True
    
    def get_consolidated_score(self) -> float:
        """Calcule le score consolid√© sur tous les tests."""
        if not self.results:
            return 0.0
        total_score = sum(r.overall_score for r in self.results)
        return total_score / len(self.results)


# ============================================================================
# TESTS (progression de difficult√©)
# ============================================================================

TEST_CASES = [
    {
        "num": 1,
        "name": "Position Simple",
        "model": SimplePosition,
        "prompt": """Extrais les informations suivantes en JSON:
        J'ai 100 actions d'Airbus (AIR.PA) √† 120‚Ç¨ par action.""",
        "expected_fields": ["symbole", "quantite", "prix"]
    },
    {
        "num": 2,
        "name": "Portfolio Simple",
        "model": SimplePortfolio,
        "prompt": """Extrais le portfolio suivant en JSON:
        Mon portfolio:
        - 50 actions Airbus (AIR.PA) √† 120‚Ç¨
        - 30 actions Sanofi (SAN.PA) √† 85‚Ç¨
        - 100 actions TotalEnergies (TTE.PA) √† 55‚Ç¨
        Valeur totale: 18,500‚Ç¨""",
        "expected_fields": ["positions", "valeur_totale"]
    },
    {
        "num": 3,
        "name": "Portfolio D√©taill√©",
        "model": PortfolioDetaille,
        "prompt": """Extrais le portfolio d√©taill√© suivant en JSON:
        Portfolio au 1er novembre 2024:
        - 50 actions Airbus (AIR.PA), secteur a√©ronautique, France, achet√©es √† 120‚Ç¨ le 15/03/2024
        - 30 actions Sanofi (SAN.PA), secteur pharmaceutique, France, achet√©es √† 85‚Ç¨ le 20/02/2024
        - 100 actions TotalEnergies (TTE.PA), secteur √©nergie, France, achet√©es √† 55‚Ç¨ le 10/01/2024
        Valeur totale: 18,500‚Ç¨""",
        "expected_fields": ["positions", "valeur_totale", "date_evaluation"]
    },
    {
        "num": 4,
        "name": "Position avec Performance",
        "model": PositionAvecPerformance,
        "prompt": """Extrais les donn√©es suivantes en JSON:
        Position: 100 actions TotalEnergies (TTE.PA) achet√©es √† 55‚Ç¨
        Performance mensuelle:
        - Janvier 2024: rendement 5%, volatilit√© 12%
        - F√©vrier 2024: rendement -2%, volatilit√© 15%
        - Mars 2024: rendement 8%, volatilit√© 10%""",
        "expected_fields": ["symbole", "quantite", "prix_achat", "performances"]
    },
    {
        "num": 5,
        "name": "Historique de Transactions",
        "model": HistoriqueTransactions,
        "prompt": """Extrais l'historique de transactions suivant en JSON:
        Transactions:
        - Achat: 50 AIR.PA √† 120‚Ç¨ le 15/03/2024, frais 5‚Ç¨
        - Achat: 30 SAN.PA √† 85‚Ç¨ le 20/02/2024, frais 3‚Ç¨
        - Vente: 20 AIR.PA √† 125‚Ç¨ le 10/04/2024, frais 2‚Ç¨
        Total achats: 8,508‚Ç¨, Total ventes: 2,498‚Ç¨, Solde net: -6,010‚Ç¨""",
        "expected_fields": ["transactions", "total_achats", "total_ventes", "solde_net"]
    },
    {
        "num": 6,
        "name": "Portfolio avec Analyse",
        "model": PortfolioAnalyse,
        "prompt": """Extrais et analyse le portfolio suivant en JSON:
        Portfolio:
        - 50 AIR.PA achet√©es √† 120‚Ç¨, prix actuel 125‚Ç¨
          Analyse: volatilit√© 18%, beta 1.2, Sharpe 0.8, VaR 95% = 2,500‚Ç¨
          Performance: 1m=5%, 3m=12%, 1y=25%
        - 30 SAN.PA achet√©es √† 85‚Ç¨, prix actuel 88‚Ç¨
          Analyse: volatilit√© 12%, beta 0.8, Sharpe 1.1, VaR 95% = 1,200‚Ç¨
          Performance: 1m=3%, 3m=8%, 1y=15%
        Analyse globale: volatilit√© portfolio 15%, Sharpe 0.95
        Recommandations: Diversifier davantage, r√©duire exposition √©nergie""",
        "expected_fields": ["positions", "analyse_globale", "recommandations"]
    },
    {
        "num": 7,
        "name": "Position Flexible avec Champs Optionnels",
        "model": PositionFlexible,
        "prompt": """Extrais la position suivante en JSON (certains champs peuvent √™tre optionnels):
        Position: 100 TTE.PA achet√©es √† 55‚Ç¨
        Prix actuel: 58‚Ç¨
        M√©triques: P/E ratio 12.5 (unit√©: multiple), Beta 1.1
        Tags: √©nergie, dividende, stable""",
        "expected_fields": ["symbole", "quantite", "prix_achat"]
    },
    {
        "num": 8,
        "name": "Calculs Financiers Complexes",
        "model": AnalyseCalculs,
        "prompt": """Extrais et valide les calculs suivants en JSON:
        Calcul 1: Valeur future
        - Capital initial: 10,000‚Ç¨
        - Taux: 5% annuel
        - Dur√©e: 10 ans
        - R√©sultat: 16,289‚Ç¨
        - D√©tails: int√©r√™ts 6,289‚Ç¨
        
        Calcul 2: Versement mensuel
        - Capital: 200,000‚Ç¨
        - Taux: 3% annuel
        - Dur√©e: 20 ans (240 mois)
        - R√©sultat: 1,109‚Ç¨/mois
        - D√©tails: total rembours√© 266,160‚Ç¨, co√ªt 66,160‚Ç¨
        
        Conclusion: Les calculs sont coh√©rents. Confiance: 0.95""",
        "expected_fields": ["calculs", "conclusion", "confiance"]
    },
    {
        "num": 9,
        "name": "Portfolio Relationnel",
        "model": PortfolioRelationnel,
        "prompt": """Extrais le portfolio avec relations en JSON:
        Positions:
        - 50 AIR.PA (a√©ronautique, France) achet√©es √† 120‚Ç¨ le 15/03/2024
        - 30 SAN.PA (pharma, France) achet√©es √† 85‚Ç¨ le 20/02/2024
        - 100 TTE.PA (√©nergie, France) achet√©es √† 55‚Ç¨ le 10/01/2024
        
        Relations:
        - AIR.PA et SAN.PA: corr√©lation faible (0.2)
        - TTE.PA et AIR.PA: corr√©lation mod√©r√©e (0.4)
        - SAN.PA et TTE.PA: corr√©lation n√©gative (-0.1) - effet de diversification
        
        Clusters: 
        - "d√©fensif": [SAN.PA]
        - "cyclique": [AIR.PA, TTE.PA]
        
        Strat√©gie: Diversification sectorielle avec biais d√©fensif""",
        "expected_fields": ["positions", "relations", "strategie"]
    },
    {
        "num": 10,
        "name": "Analyse Compl√®te du Portfolio",
        "model": AnalyseComplete,
        "prompt": """Extrais l'analyse compl√®te suivante en JSON:
        Portfolio au 1er novembre 2024:
        
        Positions:
        - 50 AIR.PA achet√©es √† 120‚Ç¨, actuel 125‚Ç¨
          Analyse risque: volatilit√© 18%, beta 1.2, Sharpe 0.8, VaR 2,500‚Ç¨
          Performance: 1m=5%, 3m=12%, 1y=25%
        - 30 SAN.PA achet√©es √† 85‚Ç¨, actuel 88‚Ç¨
          Analyse risque: volatilit√© 12%, beta 0.8, Sharpe 1.1, VaR 1,200‚Ç¨
          Performance: 1m=3%, 3m=8%, 1y=15%
        
        M√©triques globales:
        - Volatilit√© portfolio: valeur 15%, historique [14%, 15%, 16%, 15%], tendance stable, seuil 20%
        - Sharpe Ratio: valeur 0.95, historique [0.9, 0.92, 0.95, 0.94], tendance hausse, seuil 0.8
        - VaR 95%: valeur 3,500‚Ç¨, historique [3,200, 3,400, 3,500, 3,450], tendance hausse, seuil 5,000‚Ç¨
        
        Analyse risque globale: volatilit√© 15%, beta 1.0, Sharpe 0.95, VaR 3,500‚Ç¨
        
        Recommandations:
        - Diversifier davantage (actuellement 3 positions)
        - R√©duire exposition √©nergie (TTE.PA repr√©sente 30% du portfolio)
        - Augmenter allocation d√©fensive (SAN.PA)
        
        Sc√©narios:
        - Optimiste: rendement +20%, volatilit√© 12%
        - Base: rendement +10%, volatilit√© 15%
        - Pessimiste: rendement -5%, volatilit√© 18%
        
        Version mod√®le: 1.0""",
        "expected_fields": ["positions", "metriques_globales", "analyse_risque", "recommandations", "date_analyse"]
    }
]


# ============================================================================
# EX√âCUTION DES TESTS
# ============================================================================

async def run_test_suite():
    """Ex√©cute la suite compl√®te de tests."""
    print("=" * 80)
    print("SUITE DE TESTS: √âvaluation de la gestion des sorties JSON")
    print("=" * 80)
    print()
    
    judge = JSONJudge()
    
    for test_case in TEST_CASES:
        print(f"\n{'='*80}")
        print(f"TEST {test_case['num']}/10: {test_case['name']}")
        print(f"{'='*80}")
        print(f"Prompt: {test_case['prompt'][:100]}...")
        print()
        
        try:
            # Ex√©cuter l'agent avec output_type pour validation automatique
            result = await extract_agent.run(
                test_case['prompt'],
                output_type=test_case['model']
            )
            
            # Extraire le JSON de la r√©ponse
            # result.data existe seulement si output_type a r√©ussi la validation automatique
            # Sinon, result.data n'existe pas et on utilise result.output
            response_json = None
            validated_data = None

            # Essayer d'acc√©der √† result.data (existe seulement si validation r√©ussie)
            try:
                result_data = result.data
                if result_data is not None:
                    # Validation automatique r√©ussie, data est un objet Pydantic valid√©
                    if hasattr(result_data, 'model_dump_json'):
                        response_json = result_data.model_dump_json()
                        validated_data = result_data
                    elif hasattr(result_data, 'model_dump'):
                        response_json = json.dumps(result_data.model_dump())
                        validated_data = result_data
                    else:
                        response_json = json.dumps(result_data)
                        validated_data = result_data
            except AttributeError:
                # result.data n'existe pas - validation a √©chou√© ou pas utilis√©e
                response_json = result.output
                validated_data = None
            
            print(f"‚úÖ R√©ponse re√ßue (longueur: {len(response_json)} caract√®res)")
            try:
                json_preview = json.dumps(json.loads(response_json), indent=2, ensure_ascii=False)
                # Limiter l'affichage pour les grandes structures
                if len(json_preview) > 500:
                    json_preview = json_preview[:500] + "\n... (tronqu√©)"
                print(f"üìÑ JSON g√©n√©r√©:\n{json_preview}")
            except:
                print(f"üìÑ JSON g√©n√©r√© (affichage brut):\n{response_json[:200]}...")
            
            # √âvaluer avec le juge
            # Si validated_data existe, le sch√©ma est d√©j√† valid√© par PydanticAI
            judge_result = judge.evaluate(
                test_num=test_case['num'],
                test_name=test_case['name'],
                expected_model=test_case['model'],
                response_text=response_json,
                expected_fields=test_case.get('expected_fields')
            )
            
            # Si output_type a r√©ussi, le sch√©ma est forc√©ment valide
            if validated_data is not None:
                judge_result.schema_valid = True
                judge_result.semantics_valid = True
                if judge_result.overall_score < 0.8:
                    judge_result.overall_score = 0.95  # Bonus pour validation automatique r√©ussie
            
            judge.results.append(judge_result)
            
            # Afficher les r√©sultats
            print(f"\nüìä R√©sultats d'√©valuation:")
            print(f"  Structure JSON valide: {'‚úÖ' if judge_result.structure_valid else '‚ùå'}")
            print(f"  Sch√©ma Pydantic valide: {'‚úÖ' if judge_result.schema_valid else '‚ùå'}")
            print(f"  S√©mantique valide: {'‚úÖ' if judge_result.semantics_valid else '‚ùå'}")
            print(f"  Score compl√©tude: {judge_result.completeness_score:.2%}")
            print(f"  Score exactitude: {judge_result.correctness_score:.2%}")
            print(f"  Score global: {judge_result.overall_score:.2%}")
            
            if judge_result.errors:
                print(f"  ‚ùå Erreurs: {', '.join(judge_result.errors)}")
            if judge_result.warnings:
                print(f"  ‚ö†Ô∏è  Avertissements: {', '.join(judge_result.warnings)}")
            
        except ToolRetryError as e:
            # Le mod√®le a √©chou√© √† produire un JSON valide apr√®s plusieurs tentatives
            print(f"‚ùå √âchec de validation apr√®s retries: {str(e)}")
            # Essayer d'extraire le dernier output si disponible
            response_json = "√âchec de g√©n√©ration JSON valide"
            try:
                if hasattr(e, 'result') and e.result:
                    response_json = e.result.output
            except:
                pass

            # Cr√©er un r√©sultat d'√©chec
            judge_result = JudgeResult(
                test_num=test_case['num'],
                test_name=test_case['name'],
                structure_valid=False,
                schema_valid=False,
                semantics_valid=False,
                completeness_score=0.0,
                correctness_score=0.0,
                overall_score=0.0,
                errors=[f"ToolRetryError: Le mod√®le n'a pas pu produire un JSON valide apr√®s plusieurs tentatives"],
                warnings=[],
                details={"exception": str(e), "response_text": response_json}
            )
            judge.results.append(judge_result)
            continue
        except ValidationError as e:
            print(f"‚ùå Erreur de validation Pydantic: {str(e)}")
            # Cr√©er un r√©sultat d'√©chec avec d√©tails de validation
            # ValidationError √† ce niveau indique un √©chec de parsing/structure
            judge_result = JudgeResult(
                test_num=test_case['num'],
                test_name=test_case['name'],
                structure_valid=False,  # √âchec de parsing/structure de la r√©ponse
                schema_valid=False,
                semantics_valid=False,
                completeness_score=0.0,
                correctness_score=0.0,
                overall_score=0.0,  # Score √† 0 car structure invalide
                errors=[f"√âchec de parsing/structure: Validation Pydantic √©chou√©e: {str(e)}"],
                warnings=[],
                details={"validation_errors": [str(err) for err in e.errors()], "parsing_failed": True}
            )
            judge.results.append(judge_result)
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution du test: {str(e)}")
            traceback.print_exc()
            # Cr√©er un r√©sultat d'√©chec
            judge_result = JudgeResult(
                test_num=test_case['num'],
                test_name=test_case['name'],
                structure_valid=False,
                schema_valid=False,
                semantics_valid=False,
                completeness_score=0.0,
                correctness_score=0.0,
                overall_score=0.0,
                errors=[f"Erreur d'ex√©cution: {str(e)}"],
                warnings=[],
                details={"exception": str(e), "traceback": traceback.format_exc()}
            )
            judge.results.append(judge_result)
    
    # Afficher le score consolid√©
    print(f"\n\n{'='*80}")
    print("R√âSULTATS CONSOLID√âS")
    print(f"{'='*80}\n")
    
    consolidated_score = judge.get_consolidated_score()
    print(f"üìà Score consolid√© global: {consolidated_score:.2%}\n")
    
    print("D√©tail par test:")
    print("-" * 80)
    for result in judge.results:
        status = "‚úÖ" if result.overall_score >= 0.8 else "‚ö†Ô∏è" if result.overall_score >= 0.5 else "‚ùå"
        print(f"{status} Test {result.test_num:2d}: {result.test_name:40s} | Score: {result.overall_score:6.2%}")
    
    print(f"\n{'='*80}")
    print(f"Score final: {consolidated_score:.2%}")
    print(f"{'='*80}\n")
    
    return judge


if __name__ == "__main__":
    asyncio.run(run_test_suite())

